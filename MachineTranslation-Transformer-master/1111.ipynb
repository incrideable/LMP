{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from keras_transformer import get_model, decode\n",
    "# main_path = '/content/drive/My Drive/Colab Notebooks/'    #Google Colab FilePath\n",
    "main_path = './'\n",
    "path = main_path + 'middle_data/'\n",
    "path = 'middle_data/'\n",
    "with open(path + 'encode_input.pkl', 'rb') as f:\n",
    "    encode_input = pickle.load(f)\n",
    "with open(path + 'decode_input.pkl', 'rb') as f:\n",
    "    decode_input = pickle.load(f)\n",
    "with open(path + 'decode_output.pkl', 'rb') as f:\n",
    "    decode_output = pickle.load(f)\n",
    "with open(path + 'source_token_dict.pkl', 'rb') as f:\n",
    "    source_token_dict = pickle.load(f)\n",
    "with open(path + 'target_token_dict.pkl', 'rb') as f:\n",
    "    target_token_dict = pickle.load(f)\n",
    "with open(path + 'source_tokens.pkl', 'rb') as f:\n",
    "    source_tokens = pickle.load(f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2752,
     "status": "ok",
     "timestamp": 1588303929770,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "em3y9E2S6Too",
    "outputId": "71c49f8b-a654-46c4-d381-76495a22b272"
   },
   "outputs": [],
   "source": [
    "print(len(source_token_dict))\n",
    "print(len(target_token_dict))\n",
    "print(len(encode_input))\n",
    "# 构建模型\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=64,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # 不同语言需要使用不同的词嵌入\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "# model.summary()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2704582,
     "status": "ok",
     "timestamp": 1588306654812,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "bezIJiRX6Tox",
    "outputId": "a9eda80b-c48b-402d-f43f-72252a193f5d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def clean_sequences(sequences):\n",
    "    cleaned = []\n",
    "    for seq in sequences:\n",
    "        if not hasattr(seq, '__len__') or isinstance(seq, (int, float)):\n",
    "            cleaned.append([int(seq)])\n",
    "        else:\n",
    "            if hasattr(seq, 'tolist'):\n",
    "                seq = seq.tolist()\n",
    "            flat = []\n",
    "            for x in seq:\n",
    "                if hasattr(x, '__len__') and not isinstance(x, str):\n",
    "                    flat.extend([int(i) for i in x])\n",
    "                else:.\n",
    "                    flat.append(int(x))\n",
    "            cleaned.append(flat)\n",
    "    return cleaned\n",
    "\n",
    "# —— 1. 构造双向训练数据 —— #\n",
    "# 中→英: encode_input -> decode_input -> decode_output\n",
    "# 英→中: decode_output -> encode_input -> encode_output (需构造)\n",
    "\n",
    "# 构造 encode_output：即原中文输入加 <END>\n",
    "end_token_id = source_token_dict['<END>']\n",
    "encode_output = [seq + [end_token_id] for seq in encode_input]\n",
    "\n",
    "# 构造训练集：合并两组对称翻译对\n",
    "encode_input_all  = encode_input + decode_output\n",
    "decode_input_all  = decode_input + encode_input\n",
    "decode_output_all = decode_output + encode_output  # 修复此处\n",
    "\n",
    "# —— 2. 动态计算并打印 MAX_LEN —— #\n",
    "lens_enc     = [len(seq) for seq in encode_input_all]\n",
    "lens_dec_in  = [len(seq) for seq in decode_input_all]\n",
    "lens_dec_out = [len(seq) for seq in decode_output_all]\n",
    "MAX_LEN = max(max(lens_enc), max(lens_dec_in), max(lens_dec_out))\n",
    "print(\"使用的 MAX_LEN =\", MAX_LEN)\n",
    "\n",
    "# —— 3. 清洗输出列表中可能的标量或嵌套样本 —— #\n",
    "cleaned = []\n",
    "for seq in decode_output_all:\n",
    "    if not hasattr(seq, '__len__') or isinstance(seq, (int, float)):\n",
    "        cleaned.append([int(seq)])\n",
    "    else:\n",
    "        if hasattr(seq, 'tolist'):\n",
    "            seq = seq.tolist()\n",
    "        flat = []\n",
    "        for x in seq:\n",
    "            if hasattr(x, '__len__') and not isinstance(x, str):\n",
    "                flat.extend([int(i) for i in x])\n",
    "            else:\n",
    "                flat.append(int(x))\n",
    "        cleaned.append(flat)\n",
    "decode_output_all = cleaned\n",
    "\n",
    "# 清洗三个序列\n",
    "encode_input_all = clean_sequences(encode_input_all)\n",
    "decode_input_all = clean_sequences(decode_input_all)\n",
    "decode_output_all = clean_sequences(decode_output_all)\n",
    "\n",
    "# —— 4. pad_sequences —— #\n",
    "pad_enc_id = source_token_dict['<PAD>']\n",
    "pad_dec_id = target_token_dict['<PAD>']\n",
    "\n",
    "encode_input_all = pad_sequences(\n",
    "    encode_input_all, maxlen=MAX_LEN,\n",
    "    padding='post', truncating='post', value=pad_enc_id\n",
    ")\n",
    "decode_input_all = pad_sequences(\n",
    "    decode_input_all, maxlen=MAX_LEN,\n",
    "    padding='post', truncating='post', value=pad_dec_id\n",
    ")\n",
    "decode_output_all = pad_sequences(\n",
    "    decode_output_all, maxlen=MAX_LEN,\n",
    "    padding='post', truncating='post', value=pad_dec_id\n",
    ")\n",
    "\n",
    "# —— 5. 定义回调 —— #\n",
    "filepath = \"./models/W--{epoch:03d}-{loss:.4f}-.weights.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='min',\n",
    "    save_weights_only=True, save_freq='epoch'\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss', factor=0.2, patience=2,\n",
    "    verbose=1, mode='min', min_delta=1e-4, min_lr=0\n",
    ")\n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "# —— 6. 训练 —— #\n",
    "history = model.fit(\n",
    "    x=[encode_input_all, decode_input_all],\n",
    "    y=decode_output_all,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list\n",
    ")\n",
    "for i in range(3):\n",
    "    print(\"Enc:\", encode_input_all[i])\n",
    "    print(\"Dec In:\", decode_input_all[i])\n",
    "    print(\"Dec Out:\", decode_output_all[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8343,
     "status": "ok",
     "timestamp": 1588309168109,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "DIPdi5jJ6To3",
    "outputId": "fb4ff73c-2b7b-409d-88fa-ef1a5581b159"
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model.load_weights('models/W--010-0.5277-.weights.h5')\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155461,
     "status": "ok",
     "timestamp": 1588309337115,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "waCFmuVtjRn5",
    "outputId": "4f15a6a7-e0cb-4857-dcaa-8759a706d86d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import jieba\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "# 判断是否包含中文字符\n",
    "def contains_chinese(text):\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# 获取输入序列并转换为编码（支持中英互译）\n",
    "def get_input(seq, is_chinese):\n",
    "    if is_chinese:\n",
    "        seq = ' '.join(jieba.lcut(seq, cut_all=False))\n",
    "        seq = seq.split(' ')\n",
    "        token_dict = source_token_dict\n",
    "    else:\n",
    "        seq = seq.strip().split(' ')\n",
    "        token_dict = target_token_dict\n",
    "\n",
    "    seq = ['<START>'] + seq + ['<END>']\n",
    "    seq = seq + ['<PAD>'] * (34 - len(seq))\n",
    "    \n",
    "    for x in seq:\n",
    "        if x not in token_dict:\n",
    "            return False, []\n",
    "    seq_ids = [token_dict[x] for x in seq]\n",
    "    return True, seq_ids\n",
    "\n",
    "# 翻译并输出结果\n",
    "def get_ans(seq_ids, is_chinese):\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        [seq_ids],\n",
    "        start_token=(target_token_dict if is_chinese else source_token_dict)['<START>'],\n",
    "        end_token=(target_token_dict if is_chinese else source_token_dict)['<END>'],\n",
    "        pad_token=(target_token_dict if is_chinese else source_token_dict)['<PAD>'],\n",
    "    )\n",
    "    token_dict_inv = target_token_dict_inv if is_chinese else {v: k for k, v in source_token_dict.items()}\n",
    "    print(' '.join(map(lambda x: token_dict_inv[x], decoded[0][1:-1])))\n",
    "\n",
    "# 循环交互\n",
    "while True:\n",
    "    seq = input(\"请输入中英文句子 (输入 'x' 退出): \")\n",
    "    if seq.strip().lower() == 'x':\n",
    "        break\n",
    "    is_chinese = contains_chinese(seq)\n",
    "    flag, seq_ids = get_input(seq, is_chinese)\n",
    "    if flag:\n",
    "        get_ans(seq_ids, is_chinese)\n",
    "    else:\n",
    "        print('听不懂呢。')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
