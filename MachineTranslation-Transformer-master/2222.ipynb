{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from keras_transformer import get_model, decode\n",
    "# main_path = '/content/drive/My Drive/Colab Notebooks/'    #Google Colab FilePath\n",
    "main_path = './'\n",
    "path = main_path + 'middle_data/'\n",
    "path = 'middle_data/'\n",
    "with open(path + 'encode_input.pkl', 'rb') as f:\n",
    "    encode_input = pickle.load(f)\n",
    "with open(path + 'decode_input.pkl', 'rb') as f:\n",
    "    decode_input = pickle.load(f)\n",
    "with open(path + 'decode_output.pkl', 'rb') as f:\n",
    "    decode_output = pickle.load(f)\n",
    "with open(path + 'source_token_dict.pkl', 'rb') as f:\n",
    "    source_token_dict = pickle.load(f)\n",
    "with open(path + 'target_token_dict.pkl', 'rb') as f:\n",
    "    target_token_dict = pickle.load(f)\n",
    "with open(path + 'source_tokens.pkl', 'rb') as f:\n",
    "    source_tokens = pickle.load(f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2752,
     "status": "ok",
     "timestamp": 1588303929770,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "em3y9E2S6Too",
    "outputId": "71c49f8b-a654-46c4-d381-76495a22b272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10565\n",
      "7307\n",
      "20403\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(len(source_token_dict))\n",
    "print(len(target_token_dict))\n",
    "print(len(encode_input))\n",
    "# 构建模型\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=64,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # 不同语言需要使用不同的词嵌入\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "# model.summary()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 TensorFlow 版本: 2.19.0\n",
      "🧠 是否有 GPU: []\n",
      "\n",
      "📦 本地设备信息:\n",
      "name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10085201896275080810\n",
      "xla_global_id: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"🔍 TensorFlow 版本:\", tf.__version__)\n",
    "print(\"🧠 是否有 GPU:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"\\n📦 本地设备信息:\")\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2704582,
     "status": "ok",
     "timestamp": 1588306654812,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "bezIJiRX6Tox",
    "outputId": "a9eda80b-c48b-402d-f43f-72252a193f5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Dropout\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === 1. 数据加载和 SentencePiece 训练 ===\n",
    "data_path = 'cmn.txt'  # 每行格式: 英文 \\t 中文\n",
    "with open(data_path, encoding='utf-8') as f:\n",
    "    lines = [l.strip() for l in f if '\\t' in l]\n",
    "pairs = [l.split('\\t') for l in lines]\n",
    "en_texts, zh_texts = zip(*pairs)\n",
    "\n",
    "# 生成混合训练文件\n",
    "with open('combined.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, zh in zip(en_texts, zh_texts):\n",
    "        f.write(f\"<to_en> {zh}\\n\")\n",
    "        f.write(f\"<to_zh> {en}\\n\")\n",
    "\n",
    "# 训练共享词表\n",
    "if not os.path.exists('spm.model'):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='combined.txt', model_prefix='spm',\n",
    "        vocab_size=16000, model_type='bpe', character_coverage=1.0,\n",
    "        control_symbols=['<PAD>', '<START>', '<END>', '<to_en>', '<to_zh>']\n",
    "    )\n",
    "\n",
    "# === 2. 加载 SentencePiece 模型 ===\n",
    "sp = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "pad_id = sp.piece_to_id('<PAD>')\n",
    "start_id = sp.piece_to_id('<START>')\n",
    "end_id = sp.piece_to_id('<END>')\n",
    "to_en_id = sp.piece_to_id('<to_en>')\n",
    "to_zh_id = sp.piece_to_id('<to_zh>')\n",
    "\n",
    "# === 3. 编码样本对 ===\n",
    "def encode_pair(en, zh):\n",
    "    zh2en_enc = [to_en_id] + sp.encode(zh, out_type=int)\n",
    "    zh2en_dec_in = [start_id] + sp.encode(en, out_type=int)\n",
    "    zh2en_dec_out = sp.encode(en, out_type=int) + [end_id]\n",
    "\n",
    "    en2zh_enc = [to_zh_id] + sp.encode(en, out_type=int)\n",
    "    en2zh_dec_in = [start_id] + sp.encode(zh, out_type=int)\n",
    "    en2zh_dec_out = sp.encode(zh, out_type=int) + [end_id]\n",
    "\n",
    "    return [\n",
    "        zh2en_enc, zh2en_dec_in, zh2en_dec_out,\n",
    "        en2zh_enc, en2zh_dec_in, en2zh_dec_out\n",
    "    ]\n",
    "\n",
    "all_enc_in, all_dec_in, all_dec_out = [], [], []\n",
    "for en, zh in zip(en_texts, zh_texts):\n",
    "    a, b, c, d, e, f = encode_pair(en, zh)\n",
    "    all_enc_in.extend([a, d])\n",
    "    all_dec_in.extend([b, e])\n",
    "    all_dec_out.extend([c, f])\n",
    "\n",
    "MAX_LEN = max(map(len, all_enc_in + all_dec_in + all_dec_out))\n",
    "print(\"MAX_LEN =\", MAX_LEN)\n",
    "\n",
    "X_enc = pad_sequences(all_enc_in, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "X_dec = pad_sequences(all_dec_in, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "Y_out = pad_sequences(all_dec_out, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "Y_out = np.expand_dims(Y_out, -1)\n",
    "\n",
    "# === 4. 构建简化 Transformer 模型 ===\n",
    "def transformer_block(inputs, heads, d_ff, dropout):\n",
    "    attn_output = MultiHeadAttention(num_heads=heads, key_dim=64)(inputs, inputs)\n",
    "    out1 = LayerNormalization()(inputs + attn_output)\n",
    "    ff = Dense(d_ff, activation='relu')(out1)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return LayerNormalization()(out1 + Dropout(dropout)(ff))\n",
    "\n",
    "def build_model(vocab_size, max_len):\n",
    "    inp_enc = Input(shape=(max_len,))\n",
    "    inp_dec = Input(shape=(max_len,))\n",
    "\n",
    "    embed = Embedding(vocab_size, 256, mask_zero=True)\n",
    "    enc = embed(inp_enc)\n",
    "    dec = embed(inp_dec)\n",
    "\n",
    "    enc = transformer_block(enc, heads=4, d_ff=512, dropout=0.1)\n",
    "    dec = transformer_block(dec, heads=4, d_ff=512, dropout=0.1)\n",
    "\n",
    "    out = Dense(vocab_size, activation='softmax')(dec)\n",
    "    model = Model([inp_enc, inp_dec], out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=sp.get_piece_size(), max_len=MAX_LEN)\n",
    "model.summary()\n",
    "\n",
    "# === 5. 模型训练 ===\n",
    "checkpoint = ModelCheckpoint('best_model.weights.h5', save_best_only=True, save_weights_only=True, monitor='loss', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    [X_enc, X_dec], Y_out,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint, reduce_lr],\n",
    "    verbose=1  # 显示每个 batch 的训练过程\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8343,
     "status": "ok",
     "timestamp": 1588309168109,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "DIPdi5jJ6To3",
    "outputId": "fb4ff73c-2b7b-409d-88fa-ef1a5581b159"
   },
   "outputs": [],
   "source": [
    "#加载模型\n",
    "model.load_weights('models/W--010-0.5277-.weights.h5')\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155461,
     "status": "ok",
     "timestamp": 1588309337115,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "waCFmuVtjRn5",
    "outputId": "4f15a6a7-e0cb-4857-dcaa-8759a706d86d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import jieba\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "# 判断是否包含中文字符\n",
    "def contains_chinese(text):\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# 获取输入序列并转换为编码（支持中英互译）\n",
    "def get_input(seq, is_chinese):\n",
    "    if is_chinese:\n",
    "        seq = ' '.join(jieba.lcut(seq, cut_all=False))\n",
    "        seq = seq.split(' ')\n",
    "        token_dict = source_token_dict\n",
    "    else:\n",
    "        seq = seq.strip().split(' ')\n",
    "        token_dict = target_token_dict\n",
    "\n",
    "    seq = ['<START>'] + seq + ['<END>']\n",
    "    seq = seq + ['<PAD>'] * (34 - len(seq))\n",
    "    \n",
    "    for x in seq:\n",
    "        if x not in token_dict:\n",
    "            return False, []\n",
    "    seq_ids = [token_dict[x] for x in seq]\n",
    "    return True, seq_ids\n",
    "\n",
    "# 翻译并输出结果\n",
    "def get_ans(seq_ids, is_chinese):\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        [seq_ids],\n",
    "        start_token=(target_token_dict if is_chinese else source_token_dict)['<START>'],\n",
    "        end_token=(target_token_dict if is_chinese else source_token_dict)['<END>'],\n",
    "        pad_token=(target_token_dict if is_chinese else source_token_dict)['<PAD>'],\n",
    "    )\n",
    "    token_dict_inv = target_token_dict_inv if is_chinese else {v: k for k, v in source_token_dict.items()}\n",
    "    print(' '.join(map(lambda x: token_dict_inv[x], decoded[0][1:-1])))\n",
    "\n",
    "# 循环交互\n",
    "while True:\n",
    "    seq = input(\"请输入中英文句子 (输入 'x' 退出): \")\n",
    "    if seq.strip().lower() == 'x':\n",
    "        break\n",
    "    is_chinese = contains_chinese(seq)\n",
    "    flag, seq_ids = get_input(seq, is_chinese)\n",
    "    if flag:\n",
    "        get_ans(seq_ids, is_chinese)\n",
    "    else:\n",
    "        print('听不懂呢。')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
