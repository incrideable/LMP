{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from keras_transformer import get_model, decode\n",
    "# main_path = '/content/drive/My Drive/Colab Notebooks/'    #Google Colab FilePath\n",
    "main_path = './'\n",
    "path = main_path + 'middle_data/'\n",
    "path = 'middle_data/'\n",
    "with open(path + 'encode_input.pkl', 'rb') as f:\n",
    "    encode_input = pickle.load(f)\n",
    "with open(path + 'decode_input.pkl', 'rb') as f:\n",
    "    decode_input = pickle.load(f)\n",
    "with open(path + 'decode_output.pkl', 'rb') as f:\n",
    "    decode_output = pickle.load(f)\n",
    "with open(path + 'source_token_dict.pkl', 'rb') as f:\n",
    "    source_token_dict = pickle.load(f)\n",
    "with open(path + 'target_token_dict.pkl', 'rb') as f:\n",
    "    target_token_dict = pickle.load(f)\n",
    "with open(path + 'source_tokens.pkl', 'rb') as f:\n",
    "    source_tokens = pickle.load(f)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2752,
     "status": "ok",
     "timestamp": 1588303929770,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "em3y9E2S6Too",
    "outputId": "71c49f8b-a654-46c4-d381-76495a22b272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10565\n",
      "7307\n",
      "20403\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(len(source_token_dict))\n",
    "print(len(target_token_dict))\n",
    "print(len(encode_input))\n",
    "# æ„å»ºæ¨¡å‹\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
    "    embed_dim=64,\n",
    "    encoder_num=2,\n",
    "    decoder_num=2,\n",
    "    head_num=4,\n",
    "    hidden_dim=256,\n",
    "    dropout_rate=0.05,\n",
    "    use_same_embed=False,  # ä¸åŒè¯­è¨€éœ€è¦ä½¿ç”¨ä¸åŒçš„è¯åµŒå…¥\n",
    ")\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "# model.summary()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” TensorFlow ç‰ˆæœ¬: 2.19.0\n",
      "ğŸ§  æ˜¯å¦æœ‰ GPU: []\n",
      "\n",
      "ğŸ“¦ æœ¬åœ°è®¾å¤‡ä¿¡æ¯:\n",
      "name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10085201896275080810\n",
      "xla_global_id: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"ğŸ” TensorFlow ç‰ˆæœ¬:\", tf.__version__)\n",
    "print(\"ğŸ§  æ˜¯å¦æœ‰ GPU:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"\\nğŸ“¦ æœ¬åœ°è®¾å¤‡ä¿¡æ¯:\")\n",
    "for device in device_lib.list_local_devices():\n",
    "    print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2704582,
     "status": "ok",
     "timestamp": 1588306654812,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "bezIJiRX6Tox",
    "outputId": "a9eda80b-c48b-402d-f43f-72252a193f5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dense, Dropout\n",
    "from keras.layers import LayerNormalization, MultiHeadAttention\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === 1. æ•°æ®åŠ è½½å’Œ SentencePiece è®­ç»ƒ ===\n",
    "data_path = 'cmn.txt'  # æ¯è¡Œæ ¼å¼: è‹±æ–‡ \\t ä¸­æ–‡\n",
    "with open(data_path, encoding='utf-8') as f:\n",
    "    lines = [l.strip() for l in f if '\\t' in l]\n",
    "pairs = [l.split('\\t') for l in lines]\n",
    "en_texts, zh_texts = zip(*pairs)\n",
    "\n",
    "# ç”Ÿæˆæ··åˆè®­ç»ƒæ–‡ä»¶\n",
    "with open('combined.txt', 'w', encoding='utf-8') as f:\n",
    "    for en, zh in zip(en_texts, zh_texts):\n",
    "        f.write(f\"<to_en> {zh}\\n\")\n",
    "        f.write(f\"<to_zh> {en}\\n\")\n",
    "\n",
    "# è®­ç»ƒå…±äº«è¯è¡¨\n",
    "if not os.path.exists('spm.model'):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='combined.txt', model_prefix='spm',\n",
    "        vocab_size=16000, model_type='bpe', character_coverage=1.0,\n",
    "        control_symbols=['<PAD>', '<START>', '<END>', '<to_en>', '<to_zh>']\n",
    "    )\n",
    "\n",
    "# === 2. åŠ è½½ SentencePiece æ¨¡å‹ ===\n",
    "sp = spm.SentencePieceProcessor(model_file='spm.model')\n",
    "pad_id = sp.piece_to_id('<PAD>')\n",
    "start_id = sp.piece_to_id('<START>')\n",
    "end_id = sp.piece_to_id('<END>')\n",
    "to_en_id = sp.piece_to_id('<to_en>')\n",
    "to_zh_id = sp.piece_to_id('<to_zh>')\n",
    "\n",
    "# === 3. ç¼–ç æ ·æœ¬å¯¹ ===\n",
    "def encode_pair(en, zh):\n",
    "    zh2en_enc = [to_en_id] + sp.encode(zh, out_type=int)\n",
    "    zh2en_dec_in = [start_id] + sp.encode(en, out_type=int)\n",
    "    zh2en_dec_out = sp.encode(en, out_type=int) + [end_id]\n",
    "\n",
    "    en2zh_enc = [to_zh_id] + sp.encode(en, out_type=int)\n",
    "    en2zh_dec_in = [start_id] + sp.encode(zh, out_type=int)\n",
    "    en2zh_dec_out = sp.encode(zh, out_type=int) + [end_id]\n",
    "\n",
    "    return [\n",
    "        zh2en_enc, zh2en_dec_in, zh2en_dec_out,\n",
    "        en2zh_enc, en2zh_dec_in, en2zh_dec_out\n",
    "    ]\n",
    "\n",
    "all_enc_in, all_dec_in, all_dec_out = [], [], []\n",
    "for en, zh in zip(en_texts, zh_texts):\n",
    "    a, b, c, d, e, f = encode_pair(en, zh)\n",
    "    all_enc_in.extend([a, d])\n",
    "    all_dec_in.extend([b, e])\n",
    "    all_dec_out.extend([c, f])\n",
    "\n",
    "MAX_LEN = max(map(len, all_enc_in + all_dec_in + all_dec_out))\n",
    "print(\"MAX_LEN =\", MAX_LEN)\n",
    "\n",
    "X_enc = pad_sequences(all_enc_in, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "X_dec = pad_sequences(all_dec_in, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "Y_out = pad_sequences(all_dec_out, maxlen=MAX_LEN, padding='post', value=pad_id)\n",
    "Y_out = np.expand_dims(Y_out, -1)\n",
    "\n",
    "# === 4. æ„å»ºç®€åŒ– Transformer æ¨¡å‹ ===\n",
    "def transformer_block(inputs, heads, d_ff, dropout):\n",
    "    attn_output = MultiHeadAttention(num_heads=heads, key_dim=64)(inputs, inputs)\n",
    "    out1 = LayerNormalization()(inputs + attn_output)\n",
    "    ff = Dense(d_ff, activation='relu')(out1)\n",
    "    ff = Dense(inputs.shape[-1])(ff)\n",
    "    return LayerNormalization()(out1 + Dropout(dropout)(ff))\n",
    "\n",
    "def build_model(vocab_size, max_len):\n",
    "    inp_enc = Input(shape=(max_len,))\n",
    "    inp_dec = Input(shape=(max_len,))\n",
    "\n",
    "    embed = Embedding(vocab_size, 256, mask_zero=True)\n",
    "    enc = embed(inp_enc)\n",
    "    dec = embed(inp_dec)\n",
    "\n",
    "    enc = transformer_block(enc, heads=4, d_ff=512, dropout=0.1)\n",
    "    dec = transformer_block(dec, heads=4, d_ff=512, dropout=0.1)\n",
    "\n",
    "    out = Dense(vocab_size, activation='softmax')(dec)\n",
    "    model = Model([inp_enc, inp_dec], out)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model(vocab_size=sp.get_piece_size(), max_len=MAX_LEN)\n",
    "model.summary()\n",
    "\n",
    "# === 5. æ¨¡å‹è®­ç»ƒ ===\n",
    "checkpoint = ModelCheckpoint('best_model.weights.h5', save_best_only=True, save_weights_only=True, monitor='loss', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2, verbose=1)\n",
    "\n",
    "model.fit(\n",
    "    [X_enc, X_dec], Y_out,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint, reduce_lr],\n",
    "    verbose=1  # æ˜¾ç¤ºæ¯ä¸ª batch çš„è®­ç»ƒè¿‡ç¨‹\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8343,
     "status": "ok",
     "timestamp": 1588309168109,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "DIPdi5jJ6To3",
    "outputId": "fb4ff73c-2b7b-409d-88fa-ef1a5581b159"
   },
   "outputs": [],
   "source": [
    "#åŠ è½½æ¨¡å‹\n",
    "model.load_weights('models/W--010-0.5277-.weights.h5')\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155461,
     "status": "ok",
     "timestamp": 1588309337115,
     "user": {
      "displayName": "Jayee Wong",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhdF_nXQgSlUtrprqDLXzf9Kn59RlonvtGF4nHZ=s64",
      "userId": "08268369137892915441"
     },
     "user_tz": -480
    },
    "id": "waCFmuVtjRn5",
    "outputId": "4f15a6a7-e0cb-4857-dcaa-8759a706d86d"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import jieba\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "# åˆ¤æ–­æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦\n",
    "def contains_chinese(text):\n",
    "    return bool(re.search(r'[\\u4e00-\\u9fff]', text))\n",
    "\n",
    "# è·å–è¾“å…¥åºåˆ—å¹¶è½¬æ¢ä¸ºç¼–ç ï¼ˆæ”¯æŒä¸­è‹±äº’è¯‘ï¼‰\n",
    "def get_input(seq, is_chinese):\n",
    "    if is_chinese:\n",
    "        seq = ' '.join(jieba.lcut(seq, cut_all=False))\n",
    "        seq = seq.split(' ')\n",
    "        token_dict = source_token_dict\n",
    "    else:\n",
    "        seq = seq.strip().split(' ')\n",
    "        token_dict = target_token_dict\n",
    "\n",
    "    seq = ['<START>'] + seq + ['<END>']\n",
    "    seq = seq + ['<PAD>'] * (34 - len(seq))\n",
    "    \n",
    "    for x in seq:\n",
    "        if x not in token_dict:\n",
    "            return False, []\n",
    "    seq_ids = [token_dict[x] for x in seq]\n",
    "    return True, seq_ids\n",
    "\n",
    "# ç¿»è¯‘å¹¶è¾“å‡ºç»“æœ\n",
    "def get_ans(seq_ids, is_chinese):\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        [seq_ids],\n",
    "        start_token=(target_token_dict if is_chinese else source_token_dict)['<START>'],\n",
    "        end_token=(target_token_dict if is_chinese else source_token_dict)['<END>'],\n",
    "        pad_token=(target_token_dict if is_chinese else source_token_dict)['<PAD>'],\n",
    "    )\n",
    "    token_dict_inv = target_token_dict_inv if is_chinese else {v: k for k, v in source_token_dict.items()}\n",
    "    print(' '.join(map(lambda x: token_dict_inv[x], decoded[0][1:-1])))\n",
    "\n",
    "# å¾ªç¯äº¤äº’\n",
    "while True:\n",
    "    seq = input(\"è¯·è¾“å…¥ä¸­è‹±æ–‡å¥å­ (è¾“å…¥ 'x' é€€å‡º): \")\n",
    "    if seq.strip().lower() == 'x':\n",
    "        break\n",
    "    is_chinese = contains_chinese(seq)\n",
    "    flag, seq_ids = get_input(seq, is_chinese)\n",
    "    if flag:\n",
    "        get_ans(seq_ids, is_chinese)\n",
    "    else:\n",
    "        print('å¬ä¸æ‡‚å‘¢ã€‚')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
