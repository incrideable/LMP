{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 挂载Google Drive（仅在Colab环境需要）\n",
    "# 本地运行时可以跳过这部分\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 安装必要的Python包\n",
    "# keras-transformer提供了Transformer模型的实现\n",
    "!pip install keras-transformer\n",
    "\n",
    "# 打印当前工作目录和中间数据目录内容\n",
    "!pwd\n",
    "!ls \"/content/drive/My Drive/Colab Notebooks/middle_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "from keras_transformer import get_model, decode  # Transformer模型实现\n",
    "\n",
    "# 设置数据路径\n",
    "# Google Colab路径（已注释）\n",
    "# main_path = '/content/drive/My Drive/Colab Notebooks/'\n",
    "# path = main_path + 'middle_data/'\n",
    "\n",
    "# 本地运行路径\n",
    "path = 'middle_data/'\n",
    "\n",
    "# 加载预处理好的数据\n",
    "# encode_input: 编码后的源语言(中文)序列\n",
    "with open(path + 'encode_input.pkl', 'rb') as f:\n",
    "    encode_input = pickle.load(f)\n",
    "    \n",
    "# decode_input: 编码后的目标语言(英文)序列(decoder输入)\n",
    "with open(path + 'decode_input.pkl', 'rb') as f:\n",
    "    decode_input = pickle.load(f)\n",
    "    \n",
    "# decode_output: 编码后的目标语言(英文)序列(decoder目标输出)\n",
    "with open(path + 'decode_output.pkl', 'rb') as f:\n",
    "    decode_output = pickle.load(f)\n",
    "    \n",
    "# source_token_dict: 源语言(中文)词表\n",
    "with open(path + 'source_token_dict.pkl', 'rb') as f:\n",
    "    source_token_dict = pickle.load(f)\n",
    "    \n",
    "# target_token_dict: 目标语言(英文)词表\n",
    "with open(path + 'target_token_dict.pkl', 'rb') as f:\n",
    "    target_token_dict = pickle.load(f)\n",
    "    \n",
    "# source_tokens: 源语言token列表\n",
    "with open(path + 'source_tokens.pkl', 'rb') as f:\n",
    "    source_tokens = pickle.load(f)\n",
    "    \n",
    "print('数据加载完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印词表大小和数据量信息\n",
    "print('源语言词表大小:', len(source_token_dict))\n",
    "print('目标语言词表大小:', len(target_token_dict))\n",
    "print('训练样本数量:', len(encode_input))\n",
    "\n",
    "# 构建Transformer模型\n",
    "model = get_model(\n",
    "    token_num=max(len(source_token_dict), len(target_token_dict)),  # 词表大小\n",
    "    embed_dim=64,         # 词嵌入维度\n",
    "    encoder_num=2,        # Encoder层数\n",
    "    decoder_num=2,        # Decoder层数\n",
    "    head_num=4,           # 注意力头数\n",
    "    hidden_dim=256,       # 前馈网络隐藏层维度\n",
    "    dropout_rate=0.05,    # Dropout率\n",
    "    use_same_embed=False, # 中英文使用不同的词嵌入矩阵\n",
    ")\n",
    "\n",
    "# 编译模型\n",
    "# 使用Adam优化器和稀疏分类交叉熵损失函数\n",
    "model.compile('adam', 'sparse_categorical_crossentropy')\n",
    "\n",
    "# 可以取消注释查看模型结构\n",
    "# model.summary()\n",
    "\n",
    "print('模型构建完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# 模型保存路径配置\n",
    "# 保存验证损失最低的模型\n",
    "filepath = path + \"models/W-\" + \"-{epoch:3d}-{loss:.4f}-.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath,\n",
    "    monitor='loss',       # 监控损失值\n",
    "    verbose=1,           # 打印日志\n",
    "    save_best_only=True, # 只保存最好的模型\n",
    "    mode='min',          # 监控指标越小越好\n",
    "    save_freq='epoch',   # 每个epoch保存一次\n",
    ")\n",
    "\n",
    "# 学习率调度器\n",
    "# 当损失停止下降时降低学习率\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss',      # 监控损失值\n",
    "    factor=0.2,          # 学习率衰减因子\n",
    "    patience=2,          # 等待epoch数\n",
    "    verbose=1,           # 打印日志\n",
    "    mode='min',          # 监控指标越小越好\n",
    "    min_delta=0.0001,    # 最小变化量\n",
    "    cooldown=0,          # 学习率减少后的等待epoch数\n",
    "    min_lr=0             # 学习率下限\n",
    ")\n",
    "\n",
    "# 回调函数列表\n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "# 开始训练\n",
    "model.fit(\n",
    "    x=[np.array(encode_input[:20000]), np.array(decode_input[:20000])],  # 输入数据\n",
    "    y=np.array(decode_output[:20000]),  # 目标数据\n",
    "    epochs=100,        # 训练轮数\n",
    "    batch_size=64,     # 批量大小\n",
    "    verbose=1,         # 显示进度条\n",
    "    callbacks=callbacks_list,  # 回调函数\n",
    "    # 其他可选参数:\n",
    "    # class_weight=None,       # 类别权重\n",
    "    # max_queue_size=5,        # 生成器队列最大尺寸\n",
    "    # workers=1,               # 使用的线程数\n",
    "    # use_multiprocessing=False, # 是否使用多进程\n",
    "    # shuffle=False,           # 是否打乱数据\n",
    "    # initial_epoch=0          # 起始epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练好的模型\n",
    "model.load_weights('model\\\\W-- 99-0.0070-.h5')\n",
    "\n",
    "# 创建目标语言词表的反向映射(从索引到单词)\n",
    "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}\n",
    "\n",
    "print('模型加载完成')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import jieba  # 中文分词\n",
    "import requests\n",
    "\n",
    "def get_input(seq):\n",
    "    \"\"\"\n",
    "    预处理输入的中文句子\n",
    "    1. 使用jieba进行分词\n",
    "    2. 添加开始和结束标记\n",
    "    3. 填充到固定长度\n",
    "    4. 转换为词表索引\n",
    "    \"\"\"\n",
    "    # 中文分词\n",
    "    seq = ' '.join(jieba.lcut(seq, cut_all=False))\n",
    "    seq = seq.split(' ')\n",
    "    print('分词结果:', seq)\n",
    "    \n",
    "    # 添加开始和结束标记\n",
    "    seq = ['<START>'] + seq + ['<END>']\n",
    "    \n",
    "    # 填充到固定长度(34)\n",
    "    seq = seq + ['<PAD>'] * (34 - len(seq))\n",
    "    print('填充后序列:', seq)\n",
    "    \n",
    "    # 检查所有token是否在词表中\n",
    "    flag = True\n",
    "    for x in seq:\n",
    "        if x not in source_token_dict:\n",
    "            flag = False\n",
    "            break\n",
    "            \n",
    "    # 转换为词表索引\n",
    "    if flag:\n",
    "        seq = [source_token_dict[x] for x in seq]\n",
    "        \n",
    "    return flag, seq\n",
    "\n",
    "def get_ans(seq):\n",
    "    \"\"\"\n",
    "    使用训练好的模型进行翻译\n",
    "    1. 使用beam search解码\n",
    "    2. 将索引转换回单词\n",
    "    \"\"\"\n",
    "    # 使用beam search解码\n",
    "    decoded = decode(\n",
    "        model,\n",
    "        [seq],  # 输入序列\n",
    "        start_token=target_token_dict['<START>'],  # 开始标记\n",
    "        end_token=target_token_dict['<END>'],     # 结束标记\n",
    "        pad_token=target_token_dict['<PAD>'],     # 填充标记\n",
    "        # top_k=10,                               # beam size\n",
    "        # temperature=1.0,                        # 采样温度\n",
    "    )\n",
    "    \n",
    "    # 将索引转换为单词并打印结果\n",
    "    print('翻译结果:', ' '.join(map(lambda x: target_token_dict_inv[x], decoded[0][1:-1])))\n",
    "\n",
    "# 交互式翻译循环\n",
    "print('输入x退出')\n",
    "while True:\n",
    "    try:\n",
    "        seq = input('请输入中文: ')\n",
    "        if seq == 'x':\n",
    "            break\n",
    "            \n",
    "        # 预处理输入\n",
    "        flag, seq = get_input(seq)\n",
    "        \n",
    "        # 如果所有token都在词表中，进行翻译\n",
    "        if flag:\n",
    "            get_ans(seq)\n",
    "        else:\n",
    "            print('错误: 包含未知词汇')\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
